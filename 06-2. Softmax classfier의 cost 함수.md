# 6-2. Softmax classifier의 cost 함수

## Where is sigmoid?

<img src="pic/multi_cf5.PNG" style="zoom:67%;" />

<img src="pic/sigmoid.PNG" style="zoom:67%;" />

- 2.0, 1.0, 0.1의 값을 다 0~1의 값으로 만들고 합산 시 1이 되게 함(확률처럼)
- 그런 함수를 Softmax라고 함
- <img src="pic/softmax.PNG" style="zoom:67%;" />

- <img src="C:\MachineLearning\pic\sigmoid2.PNG" style="zoom:67%;" />

- 최댓값 골라서 1로 만들고 나머지는 0으로 만드는 Hot Encoding을 함

## Cost Function

- <img src="pic/cf_3.PNG" style="zoom:67%;" />

- ![](pic/cost_5.PNG)

- 예측이 틀릴 때 cost가 엄청 크고 정확하면 cost가 0

## Logistic cost VS cross entropy

- <img src="pic/cf_4.PNG" style="zoom:67%;" />
- <img src="pic/cf_5.PNG" style="zoom:50%;" />

## Descent

<img src="C:\MachineLearning\pic\cf_6.PNG" style="zoom:50%;" />

