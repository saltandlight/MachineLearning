# Lab10-2 Weight Initialization

- Xavier Initialization (Glorot Initialization)
  - 동일한 초기화 방법
- He Initialization for Relu
- Code
  - load dataset
  - create network
  - define loss function
  - experiments
    - parameters
    - model
    - eager mode

## Xavier Initialization

- 출발점으로부터 loss가 가장 최저인 지점(global minimum)을 찾는 것이 목표

- loss가 복잡한 경우가 많음

  - local minimum에 빠지는 경우가 많음
  - 또는 saddle point에 도달하게 되는 경우도 있음
  - 출발점이 어디냐? 에 따라 결과가 달라질 수 있음
  - 그래서 출발점을 좋게 해주자가 컨셉

- 평균은 0, 분산은 다음과 같다.

  - Variance = 2 / (Channel_in + Channel_out)

  - channel_in: input으로 들어가는 채널 갯수
  - channel_out: output으로 들어가는 채널 갯수
  - 각각을 더해서 숫자 2에서 이걸로 나누는 것 
  - random한 분포로 weight 설정

- He Initialization은 Relu에 특화되어있음(weight 초기화법)

  - Variance= 4 / (Channel_in + Channel_out)

## Code

### Load mnist

```python 
import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import mnist # fashion_mnist, cifar10, cifar100
tf.enable_eager_execution()

def load_mnist() :
    (train_data, train_labels), (test_data, test_labels) = mnist.load_data()
    
    train_data = np.expand_dims(train_data, axis=-1) # [N, 28, 28] -> [N, 28, 28, 1]
    test_data = np.expand_dims(test_data, axis=-1) # [N, 28, 28] -> pN, 28, 28, 1]
    
    train_data, test_data = normalize(train_data, test_data) # [0~255] -> [0~1]
    
    train_labels = to_categorical(train_labels, 10) # [N, ]-> [N, 10]
    test_labels = to_categorical(test_labels, 10) # [N, ]-> [N, 10]
    
    return train_data, train_labels, test_data, test_labels

def normalize(train_data, test_data):
    train_data = train_data.astype(np.float32) / 255.0
    test_data = train_data.astype(np.float32) / 255.0
    
    return train_data, test_data
```

## Create network

```python
def flatten() :
    return tf.keras.layers.Flatten()

def dense(channel, weight_init) :
    return tf.keras.layers.Dense(units=channel, use_bias=True, kernel_initializer=weight_init)

def relu() :
    return tf.keras.layers.Activation(tf.keras.activations.relu)

class create_model(tf.keras.Model):
    def __init__(self, label_dim):
        super(create_model, self).__init__()
        
        # RandomNormal: weight 초기화 어떻게 하는지의 방법
        # relu때는 Randomnormal, he_uniform()
        # 자비어는 glorot_uniform()
        weight_init = tf.keras.initializers.RandomNormal()
        self.model = tf.keras.Sequential()
        
        self.model.add(flatten())  # [N, 28, 28, 1] -> [N, 784]
        
        for i in range(2):
            # [N, 784] -> [N, 256] -> [N, 256]
            self.model.add(dense(256, weight_init))
            self.model.add(relu())
            
        self.model.add(dense(label_dim, weight_init))  # [N, 256] -> [N, 10]
        
    def call(self, x, training=None, mask=None):
        x = self.model(x)
        
        return x
```

## Define loss

- 전혀 변경되는 것 없이  relu 때와 똑같이 이용
- parameter, model, experiments(eager mode)도 마찬가지

- 성능 차이는? 
  - Random: 85.35% -> Xavier: 96.50%
  - 10%의 성능 향상

